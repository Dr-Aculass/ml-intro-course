{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Expectation‚ÄìMaximization (EM) Algorithm</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- List the steps of the Expectation‚ÄìMaximization (EM) algorithm\n",
    "- Explain how EM can estimate any model that has latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is Expectation‚ÄìMaximization (EM) algorithm?\n",
    "------\n",
    "\n",
    "An iterative method for estimating parameters, either maximum likelihood (MLE) or maximum a posteriori (MAP).\n",
    "\n",
    "Works well the model has on unobserved latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expectation‚ÄìMaximization (EM) Algorithm  \n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"http://www.wilsonmongwe.co.za/wp-content/uploads/2015/07/400px-EM.jpg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Expectation‚ÄìMaximization (EM) Workflow\n",
    "------\n",
    "<center><img src=\"images/em_diagram.png\" width=\"50%\"/></center>\n",
    "\n",
    "- __Expectation (E)__ step: Given the current parameters of the model, estimate a probability distribution.\n",
    "- __Maximization (M)__ step: Given the current data, estimate the parameters to update the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "EM, more formally\n",
    "-----\n",
    "\n",
    "Alternates between performing:\n",
    "\n",
    "- __Expectation (E)__ step: Using the current estimate for the parameters, create function for the expectation of the log-likelihood.  \n",
    "\n",
    "- __Maximization (M)__ step: Computes parameters maximizing the expected log-likelihood found on the E step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The M parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM example: üê• or ü•ö\n",
    "------\n",
    "\n",
    "<center><img src=\"images/green.png\" width=\"30%\"/></center>\n",
    "\n",
    "We have a circular problem ‚àû‚Ä¶\n",
    "\n",
    "If only we knew the cluster centroids, we could assign the data points to the closest clusters.   \n",
    "If only we knew which clusters the data points belong to, we could compute their centroids.  \n",
    "\n",
    "How do we assign points-to-clusters and clusters-to-points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/its-a-secret-art-passed-down-1000-generations-k-means.jpg\" width=\"40%\"/></center>\n",
    "\n",
    "__k-means__, aka the kind of clustering you should always do first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is how we gonna solve it:\n",
    "-----\n",
    "\n",
    "1. We start by randomly placing cluster centroids.\n",
    "\n",
    "2. Then, we assign each data point to a cluster based on minimum distance.\n",
    "\n",
    "3. Then, we compute the centers of those new clusters and move the centroids to that position.\n",
    "\n",
    "4. Repeat step 2-3 until we get bored (or the centroids stop moving around)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/e_1.png\" width=\"55%\"/></center>\n",
    "\n",
    "Given our model (i.e., centriod location), assign data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/m_1.png\" width=\"55%\"/></center>\n",
    "\n",
    "Given our data, update model (i.e., move centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do we do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/kmeans.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book  \n",
    "p435 of Pattern Recognition and Machine Learning by Bishop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "When do stop alternating between E & M?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We satisfy a priori stopping criteria: \n",
    "\n",
    "1. Convergence (no _significant_ improvement)  \n",
    "2. Run out of budget (time or money)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM enables an extension of k-means\n",
    "-----\n",
    "\n",
    "Instead of assigning each point to just one cluster (hard clustering), EM can also be used to estimate a probability to the membership of a point in each cluster - P(cluster|point). \n",
    "\n",
    "A data point can thus belong to several clusters (though with different probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM is trying to maximize the following function:\n",
    "------\n",
    "    \n",
    "<center><img src=\"images/em_form.png\" width=\"70%\"/></center>\n",
    "\n",
    "- X is directly observed variable\n",
    "- Œ∏ parameters of model\n",
    "- Z is not directly observed / latent variable\n",
    "    - Z is a joint (related) distribution on x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM Steps\n",
    "-----\n",
    "\n",
    "1. Initialize the parameters Œ∏\n",
    "2. Compute the best values for Z given Œ∏  \n",
    "3. Use the computed values of Z to compute a better estimate for the Œ∏  \n",
    "4. Iterate steps 2 and 3 until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EM steps, stated another way\n",
    "-----\n",
    "\n",
    "1. Initialize the parameters of the models\n",
    "\n",
    "2. Repeat:\n",
    "    2. E Step: Find the posterior probabilities of the latent variable given current parameter values.\n",
    "\n",
    "    3. M Step: Reestimate the parameter values given the current posterior probabilities.\n",
    "\n",
    "1. Hope for convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What are the Z random variables in K-means?\n",
    "\n",
    "<center><img src=\"images/e_1.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The centers of the clusters, aka the values for x,y for red and blue.\n",
    "\n",
    "They are additional / missing / latent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary: Expectation‚ÄìMaximization (EM)\n",
    "----\n",
    "\n",
    "- Expectation‚ÄìMaximization (EM) Algorithm is a series of steps to find good parameter estimates when there are latent variables.\n",
    "- EM steps:\n",
    "    1. Initialize the parameter estimates\n",
    "    2. Given the current parameter estimates, find the min log likelihood for Z (data+latent variables)\n",
    "    3. Given the current data, find better parameter estimates\n",
    "    3. Repeat steps 2 & 3\n",
    "- EM can be used estimate any model with latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
