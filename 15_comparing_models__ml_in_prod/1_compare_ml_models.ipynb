{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Model Comparisons</h2></center>\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1200/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg\" width=\"52%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define \"no free lunch\" (NFL) theorem in your words\n",
    "- Explain Akaike information criterion (AIC) and Bayesian information criterion (BIC)\n",
    "- Describe when to and _when not_ to use different evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://www.azquotes.com/picture-quotes/quote-there-ain-t-no-such-thing-as-a-free-lunch-a-libertarian-movement-slogan-the-moon-is-robert-a-heinlein-103-92-60.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "<center><h2>There Ain't No Such Thing As A Free Lunch (TANSTAAFL)</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>aka, everything has a cost.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"No Free Lunch\" (NFL) theorem\n",
    "----\n",
    "\n",
    "\"Any two optimization algorithms are equivalent when their performance is averaged across all possible problems.\"\n",
    "\n",
    "A theoretical result by David Wolpert and William Macready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*0QP3OeK7BAOWGlUcDG6VSw.png\" width=\" 95%\"/></center>\n",
    "\n",
    "If an algorithm is particularly useful at solving one class of problem, then it has to perform worse on the remaining average of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A single superior black-box optimization strategy is impossible.\n",
    "-----\n",
    "\n",
    "There is not going to be single algorithms that fit perfectly fits all data.\n",
    "\n",
    "We are going to have to architect the algorithm to better fit the data (hence when there is still ML field, practitioners, and courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What do because of NFL?\n",
    "-----\n",
    "\n",
    "Try multiple models and find one that works \"best\" for a particular problem.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How we define \"best\"\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluation metric\n",
    "- Speed\n",
    "- Complexity\n",
    "- Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Cross-validation (CV) is the best way to estimate of the predictive power of a model.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are the downsides to CV?\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is slow.\n",
    "\n",
    "For example if k = 5, then it might that ~5x as long to train all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fore example, it is rare to see CV in Deep Learning. The assumption is that there is more data, thus less chance of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Training dataset performance and bias\n",
    "-------\n",
    "\n",
    "It might be computational intractable to perform CV, thus we might only use evaluation metric performance on training dataset  only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to penalize more complex models that have similar bias.\n",
    "\n",
    "In most cases, a more complex model will have lower bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information criteria for model comparison\n",
    "-----\n",
    "\n",
    "$$IC = fit + complexity$$\n",
    "\n",
    "<center>“in-sample performance plus penalty”</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Akaike information criterion (AIC)\n",
    "-----\n",
    "\n",
    "\n",
    "$$AIC = -2ln(\\hat L) + 2p$$ \n",
    "\n",
    "$\\hat L$ is the best estimate of model parameters, aka $p(y|\\hatθ, x)$  \n",
    "p is the number of model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Akaike information criterion (AIC)\n",
    "-----\n",
    "\n",
    "$$AIC = -2ln(\\hat L) + 2p$$ \n",
    "\n",
    "The best model is is the one with the minimum AIC value. \n",
    "\n",
    "AIC rewards goodness of fit, but it also includes a penalty by the number of estimated parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "AIC only performs relative model comparisons, it is not test of a null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayesian information criterion (BIC)\n",
    "----\n",
    "\n",
    "$$BIC = -2ln(\\hat L) + ln(n)p$$ \n",
    "\n",
    "$\\hat L$ is the best estimate of model parameters, aka $p(y|\\hatθ, x)$  \n",
    "p is the number of model parameters  \n",
    "n is the number of datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>AIC & BIC generally agree which relative model is best.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2> BIC penalizes more complex models more heavily.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Why should we use AIC or BIC over adjusted R²?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Adjusted R² does allow comparisons across datasets, whichever data has largest range of values will have the higher R² value.\n",
    "\n",
    "It is not always possible to calculate R² for all model types.\n",
    "\n",
    "AIC and BIC allow for comparisons across data for any model types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mix n' match techniques\n",
    "------\n",
    "\n",
    "Use AIC or BIC to select amongst a class of models (e.g., stepwise regression).\n",
    "\n",
    "Then compare classes of models with CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLarsIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "-----\n",
    "\n",
    "- Whenever possible do Cross-validation (CV)\n",
    "- Sometimes it makes sense to compare models just on training set performance\n",
    "- Then use AIC or BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Deviance Information Criterion (DIC) \n",
    "-----\n",
    "\n",
    "A hierarchical modeling generalization of the Akaike information criterion\n",
    "\n",
    "particularly useful in Bayesian model selection problems where the posterior distributions of the models have been obtained by Markov chain Monte Carlo (MCMC) simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
